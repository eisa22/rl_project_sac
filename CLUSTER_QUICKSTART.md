# Meta-World SAC Cluster Quick Start Guide

## üöÄ Setup auf dem Cluster (einmalig)

### 1. Code hochladen

```bash
# Lokal ausf√ºhren (auf deinem PC)
./upload_code_to_cluster.sh
```

### 2. Auf Cluster einloggen und W&B einrichten

```bash
# SSH zum Cluster
ssh datalab

# Navigiere zum Projekt
cd ~/metaworld_project/source/rl_project_sac/docker/cluster

# W&B Login (einmalig)
wandb login
# API Key von: https://wandb.ai/authorize

# Alternativ: setup script verwenden
bash setup_wandb.sh
```

### 3. Verifikation

```bash
# Pr√ºfe, dass Container vorhanden ist
ls -lh /share/e11704784/containers/sac_metaworld.sif

# Pr√ºfe Projekt-Struktur
ls -la ~/metaworld_project/
# Sollte zeigen: logs/ models/ wandb_cache/ source/
```

---

## üß™ Test-Training durchf√ºhren

### Quick Test (10k steps, ~2-3 min)

```bash
cd ~/metaworld_project/source/rl_project_sac/docker/cluster
sbatch quick_test.sh
```

**Monitoring w√§hrend des Jobs:**

```bash
# Job-Status pr√ºfen
squeue -u e11704784

# Live-Log verfolgen
tail -f ~/metaworld_project/logs/quick_test_<JOB_ID>.log

# GPU-Auslastung ansehen (w√§hrend Job l√§uft)
tail -f ~/metaworld_project/logs/gpu_<JOB_ID>.log
```

**Nach Job-Abschluss:**

```bash
# Vollst√§ndiges Log ansehen
cat ~/metaworld_project/logs/quick_test_<JOB_ID>.log

# GPU-Statistiken zusammenfassen
tail -n 50 ~/metaworld_project/logs/gpu_<JOB_ID>.log
```

### Standard Test (100k steps, ~15-20 min)

```bash
sbatch train_mt10_test.sh
```

---

## üèãÔ∏è Vollst√§ndiges Training starten

### MT10 Full Training (2M steps, ~6-8 Stunden)

```bash
cd ~/metaworld_project/source/rl_project_sac/docker/cluster
sbatch train_mt10_full.sh
```

**Job-Management:**

```bash
# Alle deine Jobs anzeigen
squeue -u e11704784

# Job abbrechen (falls n√∂tig)
scancel <JOB_ID>

# Job-Informationen
scontrol show job <JOB_ID>

# Ressourcennutzung nach Abschluss
sacct -j <JOB_ID> --format=JobID,JobName,Elapsed,State,MaxRSS,MaxVMSize
```

---

## üìä W&B Sync (nach Job-Abschluss)

W&B l√§uft im Offline-Mode auf dem Cluster. Nach dem Training musst du die Runs manuell hochladen:

```bash
# Auf dem Cluster: alle offline runs syncen
cd ~/metaworld_project/wandb_cache
wandb sync wandb/run-*

# Oder spezifischen Run syncen
wandb sync wandb/run-<RUN_ID>

# Alle Runs auflisten
ls -la wandb/
```

---

## üíæ Modelle herunterladen (lokal ausf√ºhren)

### Alle Modelle runterladen

```bash
# Auf deinem PC
./download_models_from_cluster.sh
```

### Spezifischen Job herunterladen

```bash
./download_models_from_cluster.sh quick_test_12345
```

### Logs und W&B-Daten herunterladen

```bash
# Logs
rsync -avP datalab:/home/e11704784/metaworld_project/logs/ ./logs_cluster/

# W&B offline runs
rsync -avP datalab:/home/e11704784/metaworld_project/wandb_cache/ ./wandb_cluster/

# Dann lokal syncen
cd wandb_cluster
wandb sync wandb/run-*
```

---

## üîç GPU-Nutzung pr√ºfen

### W√§hrend des Trainings

```bash
# Live GPU monitoring (auf Compute-Node w√§hrend Job l√§uft)
watch -n 2 nvidia-smi

# Oder: GPU-Log-Datei verfolgen
tail -f ~/metaworld_project/logs/gpu_<JOB_ID>.log
```

### Nach dem Training

```bash
# GPU-Log auswerten
cat ~/metaworld_project/logs/gpu_<JOB_ID>.log

# Durchschnittliche Auslastung berechnen
awk -F',' 'NR>1 {sum+=$3; count++} END {print "Avg GPU Util: " sum/count "%"}' \
    ~/metaworld_project/logs/gpu_<JOB_ID>.log
```

---

## ‚öôÔ∏è GPU-Tuning anpassen

Wenn die GPU-Auslastung niedrig ist (<50%), kannst du die Batch-Gr√∂√üen erh√∂hen:

```bash
cd ~/metaworld_project/source/rl_project_sac/docker/cluster

# Bearbeite .env.gpu_config
nano .env.gpu_config

# √Ñndere z.B.:
# SAC_BATCH_SIZE=1024 ‚Üí 2048
# SAC_BUFFER_SIZE=2000000 ‚Üí 3000000

# Dann Job neu starten
sbatch train_mt10_test.sh
```

---

## üêõ Troubleshooting

### Job startet nicht

```bash
# Pr√ºfe Job-Queue und Grund
squeue -u e11704784 -o "%.18i %.9P %.30j %.8u %.8T %.10M %.9l %.6D %.20R"

# Pr√ºfe Partition-Status
sinfo -p GPU-a40
```

### Container nicht gefunden

```bash
# Pr√ºfe Container-Pfad
ls -lh /share/e11704784/containers/sac_metaworld.sif

# Falls fehlt: erneut hochladen (lokal)
rsync -avP sac_metaworld.sif datalab:/share/e11704784/containers/
```

### W&B funktioniert nicht

```bash
# Pr√ºfe Login-Status
wandb status

# Erneut einloggen
wandb login

# Pr√ºfe Offline-Runs
ls -la ~/metaworld_project/wandb_cache/wandb/
```

### Speicherplatz voll

```bash
# Speichernutzung pr√ºfen
du -sh ~/metaworld_project/*
du -sh /share/e11704784/*

# Alte Logs/Models l√∂schen
rm -rf ~/metaworld_project/logs/old_*
rm -rf ~/metaworld_project/models/old_*
```

---

## üìù Wichtige Pfade

| Typ | Pfad |
|-----|------|
| Container | `/share/e11704784/containers/sac_metaworld.sif` |
| Projekt-Code | `~/metaworld_project/source/rl_project_sac` |
| Logs | `~/metaworld_project/logs/` |
| Modelle | `~/metaworld_project/models/` |
| W&B Cache | `~/metaworld_project/wandb_cache/` |
| Job Scripts | `~/metaworld_project/source/rl_project_sac/docker/cluster/` |

---

## üéØ Typischer Workflow

```bash
# 1. Code-√Ñnderungen lokal machen
# ... edit train_metaworld.py, sac_agent.py, etc. ...

# 2. Code hochladen
./upload_code_to_cluster.sh

# 3. SSH zum Cluster
ssh datalab

# 4. Schnellen Test starten
cd ~/metaworld_project/source/rl_project_sac/docker/cluster
sbatch quick_test.sh

# 5. Job-Status pr√ºfen
squeue -u e11704784

# 6. Log verfolgen
tail -f ~/metaworld_project/logs/quick_test_*.log

# 7. Falls Test erfolgreich: Full Training
sbatch train_mt10_full.sh

# 8. Nach Abschluss: W&B syncen
cd ~/metaworld_project/wandb_cache
wandb sync wandb/run-*

# 9. Modelle runterladen (zur√ºck auf deinem PC)
./download_models_from_cluster.sh

# 10. Lokal evaluieren
python play_metaworld.py --model_path models_cluster/cluster_test_12345/final_model.pt
```

---

## üöÄ Performance-Tipps

1. **GPU-Auslastung optimieren:**
   - Pr√ºfe GPU-Log: sollte >70% sein
   - Falls niedrig: erh√∂he Batch-Size in `.env.gpu_config`
   - Falls Out-of-Memory: reduziere Batch-Size oder Buffer-Size

2. **Parallele Umgebungen:**
   - Standardwert: 8 parallel envs
   - Bei niedriger CPU-Last: erh√∂he auf 16 (falls genug CPUs)

3. **Mixed Precision:**
   - Aktiviere in `.env.gpu_config`: `USE_MIXED_PRECISION=true`
   - Kann Training um ~20-30% beschleunigen

4. **Checkpoint-Intervalle:**
   - Standard: alle 50k steps
   - F√ºr lange Runs: reduziere auf 100k (spart I/O)

---

Viel Erfolg beim Training! üéâ
