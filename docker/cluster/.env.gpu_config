# =============================================================================
# GPU-Optimized Training Configuration for Meta-World SAC on A40
# =============================================================================
#
# A40 GPU Specs:
# - VRAM: 48 GB
# - CUDA Cores: 10752
# - Memory Bandwidth: 696 GB/s
# - FP32 Performance: 37.4 TFLOPS
#
# Meta-World MT10 Memory Profile (estimated):
# - Base Environment State: 39D per env
# - Task One-Hot: 10D per env
# - Total State: 49D per env
# - Action: 4D per env
# - Per-Task Replay Buffer: 100k transitions × 49+4+1+49 = ~40 MB
# - Total Buffer (10 tasks): ~400 MB
# - Actor Network (256²): ~0.5M params = ~2 MB
# - Critic Networks (1024³ × 2): ~8M params = ~32 MB
# - Batch Processing (512 samples): ~200 KB
#
# Conservative Estimate: ~500 MB total
# → Can scale MASSIVELY on A40!
# =============================================================================

# Recommended Configurations by Training Phase

# PHASE 1: Initial Testing (fast iteration)
export GPU_TEST_BATCH_SIZE=256
export GPU_TEST_BUFFER_SIZE=100000
export GPU_TEST_LEARNING_STARTS=5000

# PHASE 2: Standard Training (balanced)
export GPU_STANDARD_BATCH_SIZE=512
export GPU_STANDARD_BUFFER_SIZE=2000000
export GPU_STANDARD_LEARNING_STARTS=10000

# PHASE 3: Aggressive Training (maximum GPU utilization)
export GPU_AGGRESSIVE_BATCH_SIZE=1024
export GPU_AGGRESSIVE_BUFFER_SIZE=5000000
export GPU_AGGRESSIVE_LEARNING_STARTS=20000

# Network Architecture (McLean et al. 2025 spec)
export ACTOR_HIDDEN="256,256"
export CRITIC_HIDDEN="1024,1024,1024"

# Learning Rate Schedule
export LEARNING_RATE=3e-4
export LEARNING_RATE_DECAY=0.99  # Optional: decay per epoch

# Optimization
export GRADIENT_STEPS=-1  # Update every env step (aggressive)
export TRAIN_FREQ=1       # Update frequency

# Entropy Tuning
export ENT_COEF="auto"
export TARGET_ENTROPY="auto"

# Soft Update
export TAU=0.005

# Discount Factor
export GAMMA=0.99

# Episode Length
export MAX_EPISODE_STEPS=150

# =============================================================================
# Performance Tuning Recommendations
# =============================================================================

# For Maximum Throughput:
# - Batch Size: 1024 (fully utilize GPU memory bandwidth)
# - Buffer Size: 5M (200k per task × 10 tasks)
# - Gradient Steps: -1 (update every step, aggressive learning)
# - Use mixed precision training if supported (AMP)

# For Stability:
# - Batch Size: 512 (McLean et al. baseline)
# - Buffer Size: 2M (200k per task)
# - Gradient Steps: 1 (conservative updates)

# For Debugging:
# - Batch Size: 128
# - Buffer Size: 100k
# - Learning Starts: 1000
