# Meta-World SAC Cluster Deployment Guide

VollstÃ¤ndige Anleitung fÃ¼r das Deployment von Multi-Task SAC Training auf dem TU Wien dataLAB Cluster.

---

## ðŸ“‹ Inhaltsverzeichnis

1. [SystemÃ¼bersicht](#systemÃ¼bersicht)
2. [Unterschiede zu Isaac Lab](#unterschiede-zu-isaac-lab)
3. [Deployment-Workflow](#deployment-workflow)
4. [GPU-Optimierung](#gpu-optimierung)
5. [Training starten](#training-starten)
6. [Troubleshooting](#troubleshooting)

---

## SystemÃ¼bersicht

### Cluster-Struktur

```
/home/e11704784/
â””â”€â”€ metaworld_project/
    â”œâ”€â”€ source/
    â”‚   â””â”€â”€ rl_project_sac/
    â”‚       â”œâ”€â”€ train_metaworld.py      # Main training script
    â”‚       â”œâ”€â”€ sac_agent.py            # Custom SAC implementation
    â”‚       â”œâ”€â”€ play_metaworld.py       # Evaluation script
    â”‚       â””â”€â”€ docker/
    â”‚           â””â”€â”€ cluster/
    â”‚               â”œâ”€â”€ .env.cluster    # Cluster config
    â”‚               â”œâ”€â”€ .env.gpu_config # GPU tuning params
    â”‚               â”œâ”€â”€ run_singularity.sh
    â”‚               â”œâ”€â”€ test_simple.sh
    â”‚               â”œâ”€â”€ train_mt10_test.sh
    â”‚               â””â”€â”€ train_mt10_full.sh
    â”œâ”€â”€ logs/                           # SLURM job logs
    â”œâ”€â”€ models/                         # Trained models
    â””â”€â”€ wandb_cache/                    # W&B offline cache

/share/e11704784/
â”œâ”€â”€ containers/
â”‚   â””â”€â”€ sac_metaworld.sif              # ~15GB Singularity image
â”œâ”€â”€ metaworld_cache/
â”‚   â””â”€â”€ mujoco_cache/                  # MuJoCo asset cache
â””â”€â”€ metaworld_work/
    â””â”€â”€ <JOBID>/                       # Job-specific work directories
```

### Container-Inhalt

- **Base Image:** PyTorch 2.1.0 CUDA 12.1
- **Python:** 3.10
- **Key Packages:**
  - PyTorch 2.9.1 (upgraded for SB3 compatibility)
  - Meta-World 3.0
  - MuJoCo 3.0+
  - Gymnasium 1.1+
  - Stable-Baselines3 2.7+ (fÃ¼r Wrappers)
  - W&B 0.16+
- **Headless Rendering:** OSMesa (kein Display nÃ¶tig)

---

## Unterschiede zu Isaac Lab

| Aspekt | Isaac Lab | Meta-World SAC |
|--------|-----------|----------------|
| **Container Size** | ~30 GB | ~15 GB |
| **Physics Engine** | PhysX (NVIDIA) | MuJoCo (DeepMind) |
| **Environment Count** | 128-256 parallel | 512-1024+ parallel (leichtgewichtiger) |
| **Cache Requirements** | ~20 GB Isaac Sim Cache | ~100 MB MuJoCo Cache |
| **Rendering** | Isaac Sim RTX | MuJoCo OSMesa |
| **W&B Integration** | Optional | Eingebaut (offline-mode) |
| **Training Duration** | 3-4 min (Cartpole 500 iters) | 8-10h (MT10 2M steps) |

**Vorteil:** Einfacherer Stack, schnelleres Startup, mehr parallele Envs mÃ¶glich!

---

## Deployment-Workflow

### Schritt 1: Lokale Vorbereitung

```bash
cd /path/to/rl_project_sac

# Docker Image ist bereits gebaut
docker images | grep sac_metaworld
# sac_metaworld:latest   eedf3c4bfba5   15GB

# Zu Singularity konvertieren
./convert_to_singularity.sh
# Output: sac_metaworld.sif (~15GB)
```

### Schritt 2: Upload zum Cluster

```bash
# Container hochladen
scp sac_metaworld.sif datalab:/share/e11704784/containers/

# Code & Skripte hochladen
rsync -avP \
    --exclude='.git' \
    --exclude='__pycache__' \
    --exclude='wandb' \
    --exclude='logs' \
    --exclude='models_mt10' \
    . datalab:/home/e11704784/metaworld_project/source/rl_project_sac/
```

### Schritt 3: Cluster-Verzeichnisse erstellen

```bash
ssh datalab

# Projekt-Struktur
mkdir -p /home/e11704784/metaworld_project/{logs,models,wandb_cache}
mkdir -p /share/e11704784/{containers,metaworld_cache,metaworld_work}
mkdir -p /share/e11704784/metaworld_cache/mujoco_cache

# Permissions
chmod -R 755 /home/e11704784/metaworld_project
chmod -R 755 /share/e11704784/metaworld_cache
```

### Schritt 4: Konfiguration anpassen

```bash
cd /home/e11704784/metaworld_project/source/rl_project_sac/docker/cluster

# .env.cluster editieren
nano .env.cluster
# WICHTIG: CLUSTER_USER auf deinen User Ã¤ndern!
```

### Schritt 5: Test-Job starten

```bash
# Einfacher Container/Import-Test
sbatch test_simple.sh

# Logs live folgen
tail -f /home/e11704784/metaworld_project/logs/test_*.log
```

**Erwartete Ausgabe:**
```
[TEST] Python version: 3.10.x
[TEST] PyTorch: 2.9.1
[TEST] CUDA available: True
[TEST] CUDA device: NVIDIA A40
[TEST] CUDA memory: 48.0 GB
[TEST] Meta-World: 3.0.0
[TEST] MT10 tasks available: 10
âœ… [TEST] All tests passed successfully!
```

### Schritt 6: Training-Test (kurz)

```bash
# 100k steps (~15-20 min)
sbatch train_mt10_test.sh
```

### Schritt 7: Full Training

```bash
# 2M steps (~8-10h)
sbatch train_mt10_full.sh
```

---

## GPU-Optimierung

### A40 Capacity Planning

**VerfÃ¼gbar:** 48 GB VRAM

**Meta-World MT10 Memory Breakdown:**

| Komponente | Memory | Details |
|------------|--------|---------|
| Replay Buffers (10 tasks Ã— 200k) | ~400 MB | Per-task buffers |
| Actor Network (256Â²) | ~2 MB | 0.5M params |
| Critic Networks (1024Â³ Ã— 2) | ~32 MB | 8M params |
| Batch Processing (1024 samples) | ~500 KB | Dynamic |
| PyTorch Overhead | ~1 GB | CUDA context |
| **Total Base** | **~1.5 GB** | |
| **Available for Scaling** | **~46 GB** | |

### Aggressive Scaling MÃ¶glichkeiten

**Option 1: Massive Batch Size**
```python
batch_size = 2048  # statt 512
# Memory: ~1 MB
# Training Speed: 2-3Ã— faster
```

**Option 2: Huge Replay Buffer**
```python
buffer_size_per_task = 500_000  # statt 200k
# Total: 5M transitions = ~2 GB
# Better sample diversity
```

**Option 3: Mixed Precision Training**
```python
# In train_metaworld.py
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
# Memory savings: ~30-40%
# Speed up: ~20-30%
```

### Empfohlene Konfigurationen

**Conservative (Stable, Research-Grade):**
```bash
--batch_size 512
--buffer_size 2000000
--learning_rate 3e-4
```

**Aggressive (Maximum Throughput):**
```bash
--batch_size 2048
--buffer_size 10000000
--learning_rate 5e-4
```

**Debugging (Fast Iteration):**
```bash
--batch_size 128
--buffer_size 100000
--learning_rate 1e-3
--total_steps 10000
```

---

## Training starten

### Test Run (Validation)

```bash
cd /home/e11704784/metaworld_project/source/rl_project_sac/docker/cluster
sbatch train_mt10_test.sh
```

**Monitoring:**
```bash
# Job Status
squeue -u $USER

# Live Logs
tail -f /home/e11704784/metaworld_project/logs/train_test_*.log

# GPU Utilization
ssh a-a40-o-X  # Compute node (check squeue for node name)
nvidia-smi -l 1
```

**Erwartete Timeline:**
- 0-2 min: Container startup, imports
- 2-5 min: Environment setup, buffer init
- 5-20 min: Training loop (100k steps)
- 20-22 min: Model save, cleanup

### Full Training Run

```bash
sbatch train_mt10_full.sh
```

**Timeline (2M steps):**
- Environment Init: ~3 min
- Training (1M steps): ~4-5h
- Training (2M steps): ~8-10h
- Total: ~10h

**Checkpoints:**
- Saved every 50k steps in `/home/e11704784/metaworld_project/models/<RUN_NAME>/`
- Final model: `final_model.pt`

### W&B Logging

Training lÃ¤uft im **offline mode**. Nach Abschluss:

```bash
cd /home/e11704784/metaworld_project/wandb_cache
wandb sync

# Oder lokal nach Download:
rsync -avP datalab:/home/e11704784/metaworld_project/wandb_cache/ ./wandb_from_cluster/
cd wandb_from_cluster
wandb sync
```

### Results Abrufen

```bash
# Von lokalem Rechner:
rsync -avP datalab:/home/e11704784/metaworld_project/models/ ./trained_models/
rsync -avP datalab:/home/e11704784/metaworld_project/logs/ ./training_logs/
```

---

## Troubleshooting

### Problem: "No space left on device"

**Symptom:**
```
OSError: [Errno 28] No space left on device
```

**LÃ¶sung:**
Bereits in `run_singularity.sh` implementiert:
```bash
export APPTAINER_TMPDIR="/share/${USER}/metaworld_work/${SLURM_JOB_ID}/apptainer_tmp"
```

**Validierung:**
```bash
ssh datalab
df -h /share
# Should show ~28T available
```

### Problem: Arguments nicht Ã¼bergeben

**Symptom:**
```
AttributeError: 'NoneType' object has no attribute 'split'
```

**Ursache:** Bereits gefixt mit `printf '%q'` in `run_singularity.sh`

**Validierung:**
```bash
grep "Python Command" /home/e11704784/metaworld_project/logs/train_*.log
# Should show: python train_metaworld.py --run_name xyz --total_steps 100000
```

### Problem: W&B Login Failed

**Symptom:**
```
wandb: ERROR Unable to authenticate
```

**LÃ¶sung:**
W&B lÃ¤uft im **offline mode**. Login nicht nÃ¶tig!

```bash
# Nach Training:
cd /home/e11704784/metaworld_project/wandb_cache
wandb login  # NUR wenn sync gewÃ¼nscht
wandb sync
```

### Problem: CUDA Out of Memory

**Symptom:**
```
RuntimeError: CUDA out of memory
```

**Diagnose:**
```bash
ssh a-a40-o-X
nvidia-smi
# Check "Memory-Usage"
```

**LÃ¶sung:**
Reduziere Batch Size:
```bash
# In train script oder als Argument
--batch_size 256  # statt 512
```

### Problem: Training hÃ¤ngt

**Symptom:**
- tqdm Progress Bar stoppt
- Keine CPU/GPU Last

**Diagnose:**
```bash
ssh a-a40-o-X
ps aux | grep python
nvidia-smi

# Check ob Prozess lÃ¤uft
top
```

**HÃ¤ufige Ursache:** Deadlock in Buffer Sampling

**LÃ¶sung:** Mit kleinerem Buffer testen:
```bash
--buffer_size 100000  # statt 2M
```

### Problem: Import Errors

**Symptom:**
```
ModuleNotFoundError: No module named 'metaworld'
```

**LÃ¶sung:** Container neu bauen:
```bash
# Lokal
cd /path/to/rl_project_sac
./build_docker.sh
./convert_to_singularity.sh
scp sac_metaworld.sif datalab:/share/e11704784/containers/
```

---

## Performance Benchmarks

### Isaac Lab Comparison

| Metric | Isaac Lab (Cartpole) | Meta-World SAC (MT10) |
|--------|---------------------|----------------------|
| Container Size | 30 GB | 15 GB |
| Startup Time | 30-40s | 10-15s |
| Environment Init | 20s | 5s |
| Parallel Envs | 128 | 512+ (skalierbar) |
| Steps/Sec (A40) | ~40k | ~100k+ |
| Memory/Env | ~50 MB | ~1 MB |

**Conclusion:** Meta-World SAC ist deutlich leichtgewichtiger und schneller!

### Training Performance

**Expected on A40:**
- **100k steps:** ~10-15 min
- **500k steps:** ~45-60 min
- **1M steps:** ~90-120 min (1.5-2h)
- **2M steps:** ~480-600 min (8-10h)

**Throughput:** ~3000-4000 steps/min (~50-70 steps/sec)

---

## Best Practices

### 1. Iterative Development

```bash
# 1. Test imports & container
sbatch test_simple.sh (5 min)

# 2. Short training run
sbatch train_mt10_test.sh (20 min)

# 3. Full training
sbatch train_mt10_full.sh (10h)
```

### 2. Checkpoint Management

```bash
# Models werden gespeichert in:
/home/e11704784/metaworld_project/models/<RUN_NAME>/
â”œâ”€â”€ checkpoint_50000.pt
â”œâ”€â”€ checkpoint_100000.pt
â”œâ”€â”€ ...
â””â”€â”€ final_model.pt

# Lokal sichern:
rsync -avP datalab:/home/e11704784/metaworld_project/models/ ./backup/
```

### 3. Multi-Job Workflows

```bash
# Job 1: Seed 42
sbatch train_mt10_full.sh

# Job 2: Seed 43 (parallel!)
# Edit train_mt10_full.sh â†’ SEED=43
sbatch train_mt10_full.sh

# Job 3: Seed 44
# ...
```

Kein Konflikt, da jeder Job eigenen Work-Dir hat!

### 4. Resume Training

Falls Job abbricht:

```python
# In train_metaworld.py (manuell hinzufÃ¼gen):
parser.add_argument("--checkpoint", type=str, default=None)

# Dann in run:
bash run_singularity.sh \
    ... \
    train_metaworld.py \
    --checkpoint /path/to/checkpoint_100000.pt \
    --total_steps 2000000
```

---

## Quick Reference

### Wichtige Befehle

```bash
# Job starten
sbatch train_mt10_full.sh

# Status checken
squeue -u $USER

# Job abbrechen
scancel <JOBID>

# Logs folgen
tail -f /home/e11704784/metaworld_project/logs/train_*.log

# GPU Status
ssh a-a40-o-X
nvidia-smi

# Results downloaden
rsync -avP datalab:/home/e11704784/metaworld_project/models/ ./models/
```

### Wichtige Pfade

| Resource | Path |
|----------|------|
| Container | `/share/e11704784/containers/sac_metaworld.sif` |
| Code | `/home/e11704784/metaworld_project/source/rl_project_sac/` |
| Cluster Skripte | `~/metaworld_project/source/rl_project_sac/docker/cluster/` |
| Logs | `/home/e11704784/metaworld_project/logs/` |
| Models | `/home/e11704784/metaworld_project/models/` |
| W&B Cache | `/home/e11704784/metaworld_project/wandb_cache/` |

---

## FAQ

**Q: Kann ich mehrere Jobs gleichzeitig laufen lassen?**  
A: Ja! Jeder Job bekommt eigenen Work-Dir. Limit: GPU-Queue-KapazitÃ¤t.

**Q: Wie viel schneller ist A40 vs. mein lokaler PC?**  
A: 2-3Ã— schneller (A40 hat mehr VRAM, hÃ¶here Bandbreite).

**Q: Kann ich den Code Ã¤ndern ohne Container neu zu bauen?**  
A: Ja! Code wird per rsync repliziert. Nur bei Dependency-Ã„nderungen neu bauen.

**Q: Wo sind die TensorBoard Logs?**  
A: W&B ersetzt TensorBoard. Nach Training: `wandb sync` zum Upload.

**Q: Kann ich Multi-GPU nutzen?**  
A: Ja, mit `#SBATCH --gpus=a40:2`. Code muss Data-Parallel sein (aktuell nicht implementiert).

**Q: Was kostet eine Training-Session?**  
A: Cluster ist gratis fÃ¼r TU-Mitglieder. Fair-Use-Policy beachten!

---

**Dokumentation erstellt:** 2025-12-09  
**Status:** âœ… Ready for Deployment  
**NÃ¤chste Schritte:** Singularity Conversion â†’ Upload â†’ Test â†’ Full Training
