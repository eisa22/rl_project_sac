# Meta-World SAC (Multi-Task + Single-Task)

Dieses Repository enth√§lt eine vollst√§ndige Implementierung des **Soft Actor-Critic (SAC)** Algorithmus mit **CUDA-Support** und **Weights & Biases (W&B) Logging**.

Es unterst√ºtzt sowohl **Single-Task Reinforcement Learning (ML1)** als auch **Multi-Task Reinforcement Learning (MT3)** f√ºr die Meta-World Umgebungen.

---

## üöÄ Installation

### 1. Python-Umgebung erstellen
```bash
conda create -n metaworld_rl python=3.10
conda activate metaworld_rl
```

### 2. Ben√∂tigte Libraries installieren
```bash
pip install metaworld==2.* gymnasium wandb
```

### 3. PyTorch mit CUDA installieren

W√§hlen Sie die korrekte Version f√ºr Ihre GPU unter: https://pytorch.org/get-started/locally/

Beispiel f√ºr CUDA 12.1:
```bash
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
```


### üß† Training starten

Das Training wird √ºber `train_metaworld.py` gesteuert.  
Es gibt zwei Modi:

---

#### 1Ô∏è‚É£ Single-Task Training (ML1)

Trainiert SAC auf **einem einzelnen Task**.

Verf√ºgbare Meta-World Tasks:

- `reach-v2`
- `push-v2`
- `pick-place-v2`

> **Hinweis:**  
> Der Parameter `--run_name samuel_reach_bigcritic` ist **nur ein Beispiel**.  
> Bitte tragt **euren eigenen Namen** ein. 
> Dadurch k√∂nnen die Runs eindeutig zugeordnet und korrekt in **Weights & Biases** getrackt werden.

Beispiel:
```bash
python train_metaworld.py --mode single --env reach-v2 --run_name samuel_reach_bigcritic
```

### üìä Weights & Biases (W&B) Setup
1. Login

In Weights and Biases (wandb) einloggen --> API Key ist im Browser im Projekt:
```bash

wandb login
```

## üìÅ Projektdateien ‚Äì √úbersicht

### `train_metaworld.py`
Das Haupt-Trainingsskript.  
Es erm√∂glicht:

- **Single-Task Training (ML1)** z. B. `reach-v2`, `push-v2`
- **Multi-Task Training (MT3)** mit 3 Tasks gleichzeitig
- automatisches Logging in **Weights & Biases**
- Ausf√ºhren von SAC-Updates und regelm√§√üiger Evaluation

Dieses Skript wird genutzt, um neue Modelle zu trainieren.

---

### `sac_agent.py`
Implementiert den eigentlichen **Soft Actor-Critic (SAC)** Algorithmus:

- Actor-Netzwerk (Policy)
- zwei gro√üe Critic-Netzwerke (Q-Funktionen)
- Target Networks
- Replay Buffer
- Entropy-Tuning
- CUDA-Support
- Logging der Trainingsmetriken

Dieses File enth√§lt die lernenden Komponenten des Agents.

---

### `play_metaworld.py` 
Skript zur **Evaluation eines trainierten Modells**:

- l√§dt ein gespeichertes SB3-Modell (SAC/TD3/DDPG)
- f√ºhrt mehrere Episoden im ausgew√§hlten Meta-World Task aus
- zeigt das Verhalten im **Rendering-Fenster**
- misst Erfolgsrate, Rewards und Steps

Perfekt, um schnell zu testen, wie gut ein Modell gelernt hat.


### 2. Konfiguration

    Projektname: metaworld-sac-mtrl

    Run-Name: Wird √ºber das Argument --run_name gesetzt. Bitte nutzen Sie Ihren eigenen, eindeutigen Run-Namen!

        Beispiele: --run_name samuel_bigcritic_test, --run_name lukas_actor_small

### 3. Geloggte Metriken
Kategorie	Metriken
Trainingsmetriken	q1_loss, q2_loss, actor_loss, alpha
Single-Task Eval	eval_avg_return, eval_success_rate
Multi-Task Eval	task_name_avg_return, task_name_success_rate (f√ºr jeden Task separat) und mean_success_all_tasks

Gerne anpassen :)